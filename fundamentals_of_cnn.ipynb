{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491a9ba2-9857-4208-8d33-c342948c4251",
   "metadata": {},
   "source": [
    "Q1.Difference btw Objct Detection and Object Classification.\n",
    "a. Explain the difference between object detection and object classification in the\n",
    "context of computer vision tasks. Provide examples to illustrate each concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8277d-d47f-4e9b-a21c-b7f297962060",
   "metadata": {},
   "source": [
    "Object detection and object classification are two fundamental tasks in the field of computer vision, and they have distinct purposes and methodologies. Here's a breakdown of the key differences between the two:\n",
    "\n",
    "Object Classification:\n",
    "\n",
    "Purpose: Object classification is the task of determining the category or label of a single object within an image. In other words, it answers the question, \"What is in this image?\" It aims to identify the primary object or the most prominent object in an image.\n",
    "\n",
    "Methodology: Object classification typically involves training a machine learning model, such as a convolutional neural network (CNN), to learn features from images and predict the category or class of the object. The output is a single label or class that describes the object in the image.\n",
    "\n",
    "Example: Given an image of a dog, a cat, and a bird, an object classification model will output the label \"dog\" because it's the primary object in the image. It does not provide information about the location or number of objects in the image, only what they are.\n",
    "\n",
    "Object Detection:\n",
    "\n",
    "Purpose: Object detection is the task of identifying and localizing multiple objects within an image. It answers both the questions, \"What is in this image?\" and \"Where are these objects in the image?\" It provides bounding boxes around each detected object.\n",
    "\n",
    "Methodology: Object detection is more complex than object classification because it involves not only classifying objects but also determining their positions in the image. It typically uses models like Faster R-CNN, YOLO (You Only Look Once), or SSD (Single Shot MultiBox Detector) to simultaneously predict object classes and their locations.\n",
    "\n",
    "Example: In an image containing a dog, a cat, and a bird, an object detection model would not only classify them as \"dog,\" \"cat,\" and \"bird\" but also draw bounding boxes around each of these objects, showing their exact locations within the image.\n",
    "\n",
    "In summary, object classification deals with identifying a single, prominent object in an image and assigning a class label to it. Object detection, on the other hand, handles the detection and localization of multiple objects within an image, providing bounding boxes around each object and their corresponding class labels. Object detection is a more comprehensive and complex task that combines object classification with spatial localization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a3d7a-612b-4fea-ae60-89aa4d02432e",
   "metadata": {},
   "source": [
    "Q2.Scenarios where Object Detection is used: \n",
    "a. Describe at least three scenarios or real-world applications where object detection\n",
    "techniques are commonly used. Explain the significance of object detection in these scenarios\n",
    "and how it benefits the respective applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdda5bc-b374-4609-8188-1788d72adf8a",
   "metadata": {},
   "source": [
    "Object detection is a crucial computer vision task with a wide range of real-world applications. Here are three scenarios where object detection techniques are commonly used and their significance in these applications:\n",
    "\n",
    "Autonomous Vehicles:\n",
    "\n",
    "Significance: Object detection plays a critical role in the development of autonomous vehicles, such as self-driving cars and drones. These vehicles need to identify and locate various objects in their surroundings, including pedestrians, other vehicles, road signs, traffic lights, and obstacles. Object detection ensures the safety and efficiency of autonomous navigation by providing real-time awareness of the environment.\n",
    "Benefits:\n",
    "Improved Safety: Object detection helps autonomous vehicles make real-time decisions to avoid collisions and accidents.\n",
    "Enhanced Navigation: It assists in lane keeping, adaptive cruise control, and precise path planning.\n",
    "Traffic Management: Autonomous vehicles equipped with object detection can contribute to smoother traffic flow and reduced congestion.\n",
    "Retail and Inventory Management:\n",
    "\n",
    "Significance: In retail, object detection is used for inventory management, shelf monitoring, and enhancing the shopping experience. It helps retailers keep track of the products on their shelves, automate restocking processes, and even prevent theft.\n",
    "Benefits:\n",
    "Reduced Labor Costs: Object detection systems automate the inventory check process, reducing the need for manual stock counting.\n",
    "Improved Customer Experience: Retailers can use object detection to personalize product recommendations or provide in-store navigation assistance.\n",
    "Theft Prevention: It can identify suspicious activities and alert security personnel to potential shoplifting incidents.\n",
    "Medical Imaging:\n",
    "\n",
    "Significance: Object detection in medical imaging is used to identify and locate anatomical structures, tumors, or abnormalities in various imaging modalities like X-rays, MRIs, and CT scans. Accurate object detection aids in early disease diagnosis and treatment planning.\n",
    "Benefits:\n",
    "Early Diagnosis: Object detection algorithms can help doctors detect diseases or anomalies at their initial stages, improving patient outcomes.\n",
    "Precision Surgery: In surgical scenarios, object detection assists surgeons in locating critical structures and making precise incisions.\n",
    "Reduced Human Error: By assisting radiologists and healthcare professionals, it reduces the risk of misdiagnosis and oversights.\n",
    "These scenarios highlight the versatility and importance of object detection in various domains. Object detection techniques not only enhance automation and efficiency but also contribute to safety, accuracy, and improved decision-making in these applications, ultimately benefiting both businesses and individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f4f58-4b92-4842-be82-ae8720c05e76",
   "metadata": {},
   "source": [
    "Q3. Image Data as Structured Data:\n",
    "a. Discuss whether image data can be considered a structured form of data. Provide reasoning\n",
    "and examples to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668c2ca-d427-44fd-8994-b1256360e486",
   "metadata": {},
   "source": [
    "Image data itself is typically unstructured in the sense that it consists of a collection of pixel values that do not inherently have a specific hierarchical or relational structure. However, image data can be structured and organized in various ways for analysis and processing, and it often contains valuable information when treated as structured data.\n",
    "\n",
    "Here are some reasons and examples to support the idea that image data can be considered structured:\n",
    "\n",
    "Pixel Arrangement: In a digital image, pixels are arranged in a grid, which forms a basic structure. This grid provides information about the spatial organization of the data, making it structured. Each pixel's position conveys information about its location within the image.\n",
    "\n",
    "Color Channels: Color images typically consist of multiple color channels, such as red, green, and blue (RGB). Each channel can be considered as a separate layer of data. This separation into channels gives structure to the data, as they have specific roles and meaning.\n",
    "\n",
    "Image Metadata: Image files often contain metadata, such as EXIF data, which provides structured information about the image, including details like the date and time it was taken, the camera settings, and geolocation. This metadata adds structure to the image data.\n",
    "\n",
    "Object Detection: Image processing and computer vision techniques can be used to identify and locate objects within images. This process involves defining regions of interest, bounding boxes, and other structural elements to describe the content within the image.\n",
    "\n",
    "Image Segmentation: Image segmentation techniques divide an image into regions based on the content. This results in a structured representation of different objects or areas within the image.\n",
    "\n",
    "Image Annotations: In machine learning and computer vision tasks, images are often annotated with labels, regions of interest, or other metadata. These annotations provide a structured way to understand and analyze the image data.\n",
    "\n",
    "Image Databases: When organizing a collection of images in a database, they can be categorized, tagged, and indexed, creating a structured repository of visual data that can be queried and retrieved based on specific criteria.\n",
    "\n",
    "Deep Learning: Convolutional Neural Networks (CNNs) and other deep learning models are designed to capture and exploit the structured patterns within image data. They leverage the hierarchical structure of layers to extract features and make predictions.\n",
    "\n",
    "Image Compression: Image compression algorithms, such as JPEG, exploit the structured patterns and redundancies in image data to reduce file size while preserving visual quality.\n",
    "\n",
    "While image data is inherently different from traditional structured data like spreadsheets or databases, it can be structured and organized for analysis, interpretation, and application. The structuring of image data often involves feature extraction, object recognition, and metadata association. In this way, image data is a valuable source of structured information that can be harnessed for a wide range of tasks, from image classification and object detection to medical imaging and satellite remote sensing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e31a1-765b-4b9d-8563-e373641e3126",
   "metadata": {},
   "source": [
    "Q4.Explaniing Information in an Image for CNN:\n",
    "a. Explain how Convolutional Neural Networks (CNN) can extract and understand information\n",
    "from an image. Discuss the key components and processes involved in analyzing image data\n",
    "using CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274c5c66-d008-42b5-ae9e-70a60ad6910f",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for image analysis and processing. They have revolutionized computer vision tasks and are widely used for tasks such as image classification, object detection, segmentation, and more. CNNs can effectively extract and understand information from images through a combination of key components and processes:\n",
    "\n",
    "Convolutional Layers:\n",
    "\n",
    "Convolutional layers are the heart of CNNs. They use a set of learnable filters or kernels to perform convolution operations on the input image. These filters slide over the input, capturing local patterns and features. Each filter identifies specific features like edges, textures, or simple shapes.\n",
    "Convolution operations are essentially element-wise multiplications and summations, allowing the network to detect features in different parts of the image.\n",
    "Pooling Layers:\n",
    "\n",
    "Pooling layers reduce the spatial dimensions of the feature maps produced by convolutional layers. Common pooling methods include max-pooling and average-pooling. These layers help in reducing computational complexity and the risk of overfitting.\n",
    "Pooling retains the most important information by preserving the most dominant features from the previous layer.\n",
    "Activation Functions:\n",
    "\n",
    "Activation functions like ReLU (Rectified Linear Unit) introduce non-linearity into the model. ReLU replaces negative values with zero, helping the network learn complex patterns and non-linear relationships in the data.\n",
    "Fully Connected Layers:\n",
    "\n",
    "After several convolutional and pooling layers, fully connected layers are used to learn high-level features and make predictions. These layers connect all neurons from the previous layer to the current layer.\n",
    "Typically, the final fully connected layer produces output for image classification, and activation functions like Softmax are used to obtain probability scores for each class.\n",
    "Backpropagation and Training:\n",
    "\n",
    "CNNs are trained using backpropagation and optimization algorithms like Stochastic Gradient Descent (SGD) or its variants. During training, the network's weights are adjusted to minimize the difference between predicted outputs and ground truth labels.\n",
    "Training data, usually consisting of a large dataset of labeled images, is essential for CNNs to generalize and make accurate predictions on new, unseen data.\n",
    "Feature Hierarchy:\n",
    "\n",
    "CNNs automatically learn a hierarchical representation of features. Lower layers capture simple features like edges and textures, while higher layers represent more complex features and object parts. This feature hierarchy helps CNNs understand images at multiple levels of abstraction.\n",
    "Transfer Learning:\n",
    "\n",
    "Transfer learning is a common technique where pre-trained CNN models (e.g., VGG, ResNet, Inception, or MobileNet) are fine-tuned on a specific task with a smaller dataset. This approach leverages the knowledge the pre-trained model has gained from a large dataset and adapts it to a new task.\n",
    "Data Augmentation:\n",
    "\n",
    "Data augmentation involves applying random transformations (e.g., rotations, translations, flips) to the training data to increase model robustness and prevent overfitting.\n",
    "Regularization Techniques:\n",
    "\n",
    "Techniques like dropout and batch normalization can be used to improve the model's generalization and training stability.\n",
    "In summary, CNNs extract and understand information from images by using convolutional layers to capture local features, pooling layers to reduce spatial dimensions, non-linear activations to model complex patterns, and fully connected layers to make predictions. Training with backpropagation optimizes the network's weights, allowing it to learn feature hierarchies, which is crucial for understanding images at different levels of detail. Transfer learning, data augmentation, and regularization techniques can further enhance CNN performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73246586-6d18-4f60-a458-01076941164f",
   "metadata": {},
   "source": [
    "Q5.Flattening Images for ANN:\n",
    "a. Discuss why it is not recommended to flatten images directly and input them into an\n",
    "Artificial Neural Network (ANN) for image classification. Highlight the limitations and\n",
    "challenges associated with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20245b-645f-4e65-ae7d-43746ed3c57a",
   "metadata": {},
   "source": [
    "Flattening images and inputting them directly into an Artificial Neural Network (ANN) for image classification is not recommended due to several limitations and challenges associated with this approach:\n",
    "\n",
    "Loss of Spatial Information:\n",
    "\n",
    "Images are inherently two-dimensional, with information distributed across pixels in both the horizontal and vertical dimensions. Flattening an image into a one-dimensional vector results in the loss of spatial information. This loss can be detrimental when dealing with tasks where the spatial arrangement of features is crucial, such as object recognition and segmentation.\n",
    "Ignoring Local Patterns:\n",
    "\n",
    "ANNs trained on flattened images treat every pixel as an independent feature. This means the network cannot effectively capture local patterns, relationships, or structures in the image. Convolutional Neural Networks (CNNs) are designed to address this issue by employing convolutional layers that capture spatial patterns effectively.\n",
    "Large Number of Parameters:\n",
    "\n",
    "Flattening an image results in a long vector that can be extremely large for high-resolution images. This leads to an explosion in the number of parameters in the ANN, making it computationally expensive and prone to overfitting. Overfitting occurs when the model becomes too complex and fails to generalize to new, unseen data.\n",
    "Scalability:\n",
    "\n",
    "ANNs that accept flattened images are not easily scalable to different image sizes. Changing the input image size requires modifying the network architecture, which can be inconvenient and inefficient. In contrast, CNNs can handle various input sizes through their convolutional and pooling layers, making them more flexible.\n",
    "Inefficiency in Learning Hierarchical Features:\n",
    "\n",
    "Flattened images provide no inherent notion of hierarchy. ANNs lack the ability to automatically learn and extract hierarchical features, which are crucial for tasks like object recognition where lower layers capture basic features (e.g., edges) and higher layers represent complex patterns (e.g., object shapes).\n",
    "Limited Robustness:\n",
    "\n",
    "ANNs trained on flattened images may not be robust to variations in image scale, translation, rotation, or perspective. These models are generally not invariant to transformations, which is a critical requirement for robust image classification. In contrast, CNNs can learn invariant representations through their architecture.\n",
    "Limited Use of Pre-trained Models:\n",
    "\n",
    "Pre-trained models for image classification, which are essential for transfer learning, are primarily based on CNN architectures. Flattened input data would not be compatible with these pre-trained models, making it difficult to leverage the benefits of transfer learning.\n",
    "In summary, using ANNs with flattened images for image classification is discouraged due to the loss of spatial information, inefficiency in capturing local patterns and hierarchical features, computational inefficiency, and limited scalability. Convolutional Neural Networks (CNNs) have been specifically designed to address these issues and have become the standard approach for image-related tasks, offering improved performance, robustness, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e16195-3d5e-49a8-84a0-514ce4770afb",
   "metadata": {},
   "source": [
    "Q6.Applying CNN to th MNIST Datast: \n",
    "a. Explain why it is not necessary to apply CNN to the MNIST dataset for image classification.\n",
    "Discuss the characteristics of the MNIST dataset and how it aligns with the requirements of\n",
    "CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcd60d-0c52-49c2-9de4-8836b6ce84fd",
   "metadata": {},
   "source": [
    "It is not necessary to apply Convolutional Neural Networks (CNNs) to the MNIST dataset for image classification because MNIST is a relatively simple and small dataset that does not require the complex feature extraction capabilities that CNNs are designed for. The MNIST dataset is already well-suited for traditional machine learning and simpler deep learning methods, such as feedforward neural networks or logistic regression. Here are some reasons why CNNs are not necessary for MNIST:\n",
    "\n",
    "Simplicity of MNIST Dataset:\n",
    "\n",
    "MNIST consists of 28x28 grayscale images of handwritten digits (0-9), which are relatively simple and don't contain complex features or structures.\n",
    "The dataset is clean, well-preprocessed, and digits are centered and scaled, making them amenable to simple feature extraction methods.\n",
    "Small Image Size:\n",
    "\n",
    "The 28x28 image size is small, and each image contains only a single digit. CNNs excel at learning hierarchical features in larger, more complex images, which is overkill for MNIST.\n",
    "Low Variability:\n",
    "\n",
    "MNIST images have minimal variability in terms of orientation, scale, and lighting conditions. The digits are well-centered and occupy a significant portion of the image.\n",
    "Pixel-Level Features:\n",
    "\n",
    "The pixel values themselves can be used as features for MNIST classification. CNNs are more suited for learning hierarchical and abstract features, which are not required for this dataset.\n",
    "Computational Efficiency:\n",
    "\n",
    "CNNs are computationally expensive, especially for deep architectures, and may not be the most efficient choice for such a small and simple dataset. Traditional machine learning models can achieve high accuracy with much less computational overhead.\n",
    "However, it's worth noting that applying CNNs to MNIST is still possible and can achieve high accuracy. CNNs can learn useful hierarchical representations, even for simple datasets. The advantage of using CNNs becomes more evident when dealing with larger and more complex image datasets, where they excel in capturing features across various scales, positions, and orientations.\n",
    "\n",
    "In summary, while applying CNNs to MNIST is not necessary for basic image classification, it can serve as a good educational exercise and can demonstrate the flexibility of CNNs in handling various types of image data. For practical purposes, simpler machine learning methods are sufficient to achieve excellent results on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291f755-fed0-46f1-92d3-63d1f2a58729",
   "metadata": {},
   "source": [
    "Q7.Extractig Faturs at Local Spac:\n",
    "a. Justify why it is important to extract features from an image at the local level rather than\n",
    "considering the entire image as a whole. Discuss the advantages and insights gained by\n",
    "performing local feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25680b67-bf09-4baf-b515-965a854d27bf",
   "metadata": {},
   "source": [
    "Extracting features from an image at the local level, as opposed to considering the entire image as a whole, is a fundamental concept in computer vision and image processing. This approach offers several advantages and insights that make it a crucial technique. Here are some justifications for local feature extraction:\n",
    "\n",
    "Robustness to Variability: Images often contain complex and diverse content, such as objects of different scales, rotations, and viewpoints. By analyzing the image at the local level, we can capture and recognize local patterns and structures that are invariant to global transformations. This robustness is especially important for tasks like object recognition and tracking.\n",
    "\n",
    "Discriminative Power: Local features focus on key regions of the image, which are often the most discriminative parts for various computer vision tasks. For example, when recognizing a face, extracting local features from facial keypoints (e.g., eyes, nose, mouth) can lead to better performance than considering the entire face as a whole.\n",
    "\n",
    "Dimensionality Reduction: Local feature extraction reduces the dimensionality of the problem, making it more manageable and computationally efficient. Rather than processing a large, high-dimensional image, you work with smaller, localized patches or regions, which simplifies the task of feature representation and classification.\n",
    "\n",
    "Translation Invariance: Local features can capture information about the relative spatial arrangement of objects, making them invariant to translations. This is particularly important in tasks like image matching and object detection, where objects may appear at different locations within the image.\n",
    "\n",
    "Efficient Computation: Processing the entire image can be computationally intensive, especially for high-resolution images. Local feature extraction allows for parallelization and optimization of processing, which is important in real-time applications or on resource-constrained devices.\n",
    "\n",
    "Noise Robustness: Local features are often more robust to noise and background clutter. By focusing on local regions of interest, you can filter out irrelevant information, improving the accuracy of feature extraction.\n",
    "\n",
    "Hierarchical Information: Local features can be combined to build a hierarchical representation of the image, allowing for the modeling of complex structures. This hierarchical approach is beneficial for tasks such as scene understanding and image segmentation.\n",
    "\n",
    "Efficient Learning: Local feature extraction can make machine learning models more efficient and effective. By providing the model with a set of localized features, it can focus on learning patterns and relationships within those regions, leading to better generalization and interpretability.\n",
    "\n",
    "Interpretable Features: Local features often correspond to meaningful parts or structures within the image, which can be useful for human interpretation and visualization. This is valuable in medical imaging, forensics, and other domains where experts need to analyze the image data.\n",
    "\n",
    "In summary, extracting features from an image at the local level offers a range of benefits, including robustness to variability, discriminative power, dimensionality reduction, translation invariance, computational efficiency, noise robustness, and the ability to build hierarchical representations. These advantages are crucial for a wide range of computer vision tasks, from object recognition to image matching and segmentation, making local feature extraction an essential component of modern image analysis techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e74b55-fcd3-4420-ab6c-49dcfb3cd648",
   "metadata": {},
   "source": [
    "Q8.Importance of Covolution ad Max Pooling:\n",
    "a. Elaborate on the importance of convolution and max pooling operations in a Convolutional\n",
    "Neural Network (CNN). Explain how these operations contribute to feature extraction and\n",
    "spatial down-sampling in CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d274e-43d7-4d64-8a31-3050571f0a4a",
   "metadata": {},
   "source": [
    "Convolution and max pooling operations are fundamental building blocks of Convolutional Neural Networks (CNNs) and play a crucial role in their success for various computer vision tasks, such as image classification, object detection, and image segmentation. These operations are primarily designed to enable feature extraction and spatial down-sampling, both of which are essential for the CNN's ability to understand and recognize patterns in images.\n",
    "\n",
    "Convolution Operation:\n",
    "\n",
    "Feature Extraction: The convolution operation is used to extract local patterns and features from an input image. It involves the use of small filters or kernels that are convolved (slid) over the input image. Each filter detects specific patterns or features, such as edges, textures, or more complex structures. The convolution operation captures these patterns by performing element-wise multiplications and summing the results within a local region of the input. As the filters move across the image, they identify different local features.\n",
    "\n",
    "Hierarchical Feature Learning: In CNNs, multiple convolutional layers are stacked, and each layer consists of multiple filters with different receptive fields. The output of one layer becomes the input to the next layer, forming a hierarchical representation of features. Lower layers capture basic features like edges and gradients, while higher layers capture more complex features, combining information from lower layers. This hierarchical feature learning enables CNNs to understand increasingly abstract and high-level patterns in the data.\n",
    "\n",
    "Max Pooling Operation:\n",
    "\n",
    "Spatial Down-Sampling: Max pooling is a down-sampling operation that reduces the spatial dimensions of the feature maps produced by the convolutional layers. It works by partitioning the feature map into non-overlapping regions (e.g., 2x2 or 3x3) and selecting the maximum value from each region to create a new, smaller feature map. This process effectively reduces the spatial resolution of the feature maps while retaining the most important information.\n",
    "\n",
    "Translation Invariance: Max pooling also provides some degree of translation invariance, meaning that small changes in the position of a feature in the input image will not drastically affect the output of the max-pooled feature map. This is especially useful for tasks like object recognition, where the exact position of an object in an image may vary.\n",
    "\n",
    "The importance of these operations can be summarized as follows:\n",
    "\n",
    "Parameter Sharing: Convolutional layers use shared weights for their filters, reducing the number of parameters in the network. This parameter sharing promotes feature reuse and helps CNNs generalize better.\n",
    "\n",
    "Hierarchical Feature Hierarchy: The multi-layered architecture of CNNs allows them to capture a wide range of features, from simple to complex, which is crucial for recognizing intricate patterns and structures in images.\n",
    "\n",
    "Spatial Hierarchy: By down-sampling through max pooling, CNNs create a spatial hierarchy of features, enabling them to focus on the most salient features and reduce the computational load in higher layers.\n",
    "\n",
    "Robustness to Variations: CNNs are capable of handling variations in scale, rotation, and position, making them robust for tasks like object detection and image classification.\n",
    "\n",
    "In summary, convolution and max pooling operations are integral to the success of CNNs. They facilitate the extraction of meaningful features from input data while efficiently reducing spatial dimensions, enabling CNNs to learn hierarchical representations and recognize complex patterns in images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
